---
title: Metric/Nonmetric Sammon Mapping
author: 
    - name: Jan de Leeuw
      orcid: 0000-0003-1420-1797
      email: jan@deleeuwpdx.net
      affiliation: 
        - name: University of California Los Angeles
          city: Los Angeles
          state: CA
          url: www.ucla.edu
      license: "CC0"
date: last-modified
date-format: long
bibliography: [mypubs.bib, total.bib]
number-sections: true
pdf-engine: lualatex
keep-tex: true
format:
   pdf:
    fontsize: 12pt
    include-in-header: 
     - preamble.tex
    keep-tex: true
    link-citations: true
    documentclass: scrartcl
    number-sections: true
   html:
    fontsize: 12pt
    include-in-header: 
     - preamble.css
    keep-md: true
    number-sections: true
toc: true
toc-depth: 3
editor: source
papersize: letter
graphics: true
link-citations: true
mainfont: Times New Roman
abstract: TBD
---

```{r loadpackages, echo = FALSE}
suppressPackageStartupMessages(library(knitr, quietly = TRUE))
suppressPackageStartupMessages(library(tinytex, quietly = TRUE))
suppressPackageStartupMessages(library(CVXR, quietly = TRUE))
suppressPackageStartupMessages(library(microbenchmark, quietly = TRUE))
```
```{r matrixprint, echo = FALSE}
matrixPrint <- function(x,
                        digits = 6,
                        width = 8,
                        format = "f",
                        flag = "+") {
  print(noquote(
    formatC(
      x,
      digits = digits,
      width = width,
      format = format,
      flag = flag
    )
  ))
}
```

```{r loadfiles, echo = FALSE}
source("smacofSSSammonC.R")
source("smacofSSSammonR.R")
source("../smacofFlat/smacofSSData/ekmanData.R")
source("../smacofFlat/smacofSSData/morseData.R")
```

**Note:** This is a working manuscript which will be expanded/updated
frequently. All suggestions for improvement are welcome. All Rmd, tex,
html, pdf, R, and C files are in the public domain. Attribution will be
appreciated, but is not required. The files can be found at
<https://github.com/deleeuw/sammon> 

\sectionbreak

# Introduction

In engineering and computer science *Sammon Mapping* is a popular multidimensional scaling (MDS) method. The technique was introduced in @sammon_69. It was originally intended to map points from a higher-dimensional
Euclidean space into points in a lower-dimensional Euclidean space by approximating the given higher dimensional distances by best-fitting lower dimensional ones. The *MASS* package for R implemented the *sammon()*
function, which generalizes the original idea by allowing the higher-dimensional distances to be
replaced by any positive symmetric matrix of dissimilarities. This was again generalized in the packages *stops* (@rusch_mair_hornik_23) and *smacofx* (@rusch_deleeuw_chen_mair_25) where optimization
is over low-dimensional configations and over power transforms of the dissimilarities.

The Sammon loss function is
\begin{equation}
\sigma(X,\Delta)=\frac{1}{\mathop{\sum\sum}_{1\leq i<j\leq n}w_{ij}\delta_{ij}}\mathop{\sum\sum}_{1\leq i<j\leq n}w_{ij}\frac{(\delta_{ij}-d_{ij}(X))^2}{\delta_{ij}}.\label{eq-loss}
\end{equation}

The metric MDS problem we address in this paper is minimizing \eqref{eq-loss} over
the $n\times p$ *configurations* $X$. In the non-metric case we minimize in addition over the
symmetric non-negative and hollow matrices $\Delta:=\{\delta_{ij}\}$, where the $\delta_{ij}$ are
monotone with the given dissimilarities. 



# Properties

The usual stress loss function in smacof, due originally to @kruskal_64a and @kruskal_64b, is
\begin{equation}
\sigma(X,\Delta)=\frac{\mathop{\sum\sum}_{1\leq i<j\leq n}w_{ij}(\delta_{ij}-d_{ij}(X))^2}{\mathop{\sum\sum}_{1\leq i<j\leq n}w_{ij}\delta_{ij}^2}.\label{eq-stress}
\end{equation}
If we compare \eqref{eq-loss} and \eqref{eq-stress} we see that in Kruskal's stress
the sum of the squared residuals is normalized by dividing by the sum of the squared
dissimilarities. This means minimum stress is between zero and one, and it also prevents
us from considering the trivial zero-residual solution $X=0$ and $\Delta=0$. In Sammon
loss each residual is normalized separately, based on the idea that large dissimilarities
are somehow less reliable or stable and should be downweighted.


If $\delta_{ij}$ and $d_{ij}(X)$ are close then
\begin{equation}
\sqrt{d_{ij}(X)}-\sqrt{\delta_{ij}}\approx\frac12\frac{1}{\sqrt{\delta_{ij}}}(d_{ij}(X)-\delta_{ij}),
\end{equation}
and thus
\begin{equation}
\sum w_{ij}\frac{(\delta_{ij}-d_{ij}(X))^2}{\delta_{ij}}\approx 4\sum w_{ij}(\sqrt{\delta_{ij}}-\sqrt{d_{ij}(X)})^2\label{eq-sqstress}
\end{equation}
More robust.

# Algorithm

In metric (ratio) MDS we minimize over $X$ for fixed $\Delta$. This is a standard metric scaling problem with weights $\smash{w_{ij}\delta_{ij}^{-1}}$. We use majorization steps to decrease loss. This is identical
to the ratio sammon option in smacofx.
In the non-metric case we use alternating least squares, i.e. we alternate majorization steps with 
minimizing over $\Delta$, satisfying the ordinal constraints, for fixed $X$.
As far as I know this has not been implemented before, and is somewhat non-standard.
First, use the fact that $\sigma(\lambda X,\lambda\Delta)=\sigma(X,\Delta)$
\begin{equation}
\min_X\sigma(X,\Delta)=\min_X \sigma(X,\lambda\Delta),
\end{equation}
i.e. the problem is homogeneous of degree zero in $\Delta$. Thus we can require without loss of generality
that
\begin{equation}
\mathop{\sum\sum}_{1\leq i<j\leq n}w_{ij}\delta_{ij}=1.
\end{equation}
With this normalization we have
\begin{equation}
\sigma(X,\Delta)=1-2\mathop{\sum\sum}_{1\leq i<j\leq n}w_{ij}d_{ij}(X)+\mathop{\sum\sum}_{1\leq i<j\leq n}w_{ij}\frac{d_{ij}^2(X)}{\delta_{ij}}
\end{equation}
Thus minimizing $\omega$ for fixed $X$ over non-negative, isotone and normalized $\Delta$ means minimizing
the third term on the right. And this means minimizing a separable, differentiable, and strictly convex 
function over a polyhedral convex set.

Define the extended real valued function $f$ equal to
\begin{equation}
f(x):=\sum_{i=1}^nw_i\frac{y_i}{x_i}
\end{equation}
if $x_i>0$ for all $i$, and to $+\infty$ otherwise. Thus the effective domain $\text{dom}(f)$ is the interior of the positive orthant.  

Problem $\mathbf{P}$ is defined as minimization of $f$ over the intersection of the polyhedral convex cone $\mathcal{K}$ defined by $0<x_1\leq\cdots\leq x_n$ and the affine set $\mathcal{A}$ defined by $w'x=1$. We assume that all $w_i$ and all $y_i$ are positive, and that the $w_i$ add up to one. In the context of Sammon Mapping the $y_i$ are squared distances and the $x_i$ are the transformed dissimilarities. 

The first and second partials of $f$ are
\begin{align}
\mathcal{D}_if(x)&=-w_i\frac{y_i}{x_i^2},\\
\mathcal{D}_{ii}f(x)&=2w_i\frac{y_i}{x_i^3},
\end{align}
and $\mathcal{D}_{ik}f(x)=0$ if $i\neq k$. This shows $f$ is convex and twice differentiable on $\text{dom}(f)$, and thus on $\mathcal{K}\cap\mathcal{A}$.

First some preliminary results.

::: {#lem-bound}
\begin{equation}
\min_{x\in\mathcal{K}\cap\mathcal{A}}f(x)\leq w'y
\end{equation}
:::


::: {.proof}
The vector $x$ with $x_i=1$ for all $i$ is in $\mathcal{K}\cap\mathcal{A}$. 
:::

It follows from @lem-bound that we can add the constraint $f(x)\leq w'y$ to the minimization problem and still have the same minimum and minimizer.

::: {#lem-fine}
Suppose $y_1\leq\cdots\leq y_n$. Then
\begin{equation}
\overline x_i= \frac{\sqrt{y_i}}{\sum_{k=1}^nw_k\sqrt{y_k}}.\label{eq-well}
\end{equation}
:::
::: {.proof}
The necessary conditions for a minimum of $f$ on $\mathcal{A}$ are
\begin{equation}
-w_i\frac{y_i}{x_i^2}=\lambda w_i,\label{eq-grad}
\end{equation}
for all $i$, together with the side condition $w'x=1$. From \eqref{eq-grad} it follows that 
the solution $\overline x$ must be proportional to $\sqrt{y}$, where the square 
root can have either sign.
Of the $2^n$ solutions only one is in the effective domain of $f$, the one for which all
all square roots are taken with a positive sign. This solution is also in $\mathcal{K}$.
Applying the side condition gives \eqref{eq-well}.
:::

The next rather trivial lemma deals with the case $n=1$, in which $w,x$ and $y$ are one-element vectors,
which identify with the corresponding scalars.

::: {#lem-one}
If $n=1$ then the minimizer $\overline x$ is equal to one and the minimum is $y$.
:::
::: {.proof}
$w$ adds up to one, so $w=1$. Also $wx$ must be one, so $\overline x=1$.
:::

The following theorem is of prime importance, because it shows problem $\mathbf{P}$ can be solved with a  with a variation of the Pool Adjacent Violaters Algorithm (PAVA). For the details on PAVA, see for 
example @deleeuw_hornik_mair_A_09.

::: {#thm-pava}
Suppose $\overline{x}$ is the optimum solution. If $y_i\geq y_{i+1}$ then $\overline{x}_i=\overline{x}_{i+1}$.
:::
::: {.proof}
We show that $y_i\geq y_{i+1}$ and $\overline{x}_i<\overline{x}_{i+1}$ leads to a contradiction. A
necessary and sufficient condition for $\overline x$ to be the optimum solution is
\begin{equation}
(x-\overline x)'\mathcal{D}f(\overline x)=-\sum_{i=1}^n w_i\frac{y_i}{\overline x_i^2}(x_i-\overline x_i)=f(\overline x)-\sum_{i=1}^n w_i\frac{y_i}{\overline x_i^2}x_i\geq 0
\end{equation}
for all $x\in\mathcal{K}\cap\mathcal{A}$ (@hiriart-urruty_lemarechal_93a, Theorem 1.1.1, page 293).

Now suppose $\overline{x}_i<\overline{x}_{i+1}$. Then for $\epsilon>0$ small enough
\begin{equation}
z=\overline{x}+\epsilon\left(\frac{e_i}{w_i}-\frac{e_{i+1}}{w_{i+1}}\right)\label{eq-perturb}
\end{equation}
is also in $\mathcal{K}\cap\mathcal{A}$. In \eqref{eq-perturb} we add a small amount to $\overline x_i$
and subtract a small amount from $\overline x_{i+1}$, while leaving all other elements of 
$\overline x$ unperturbed. Since
\begin{equation}
\sum_{i=1}^n w_i\frac{y_i}{\overline x_i^2}z_i=f(\overline x)+\epsilon\left(\frac{y_i}{\overline x_i^2}-\frac{y_{i+1}}{\overline x_{i+1}^2}\right)
\end{equation}
we must have
\begin{equation}
\frac{y_i}{\overline x_i^2}\leq\frac{y_{i+1}}{\overline x_{i+1}^2}
\end{equation}
But $y_i\geq y_{i+1}$ and $\overline x_i<\overline x_{i+1}$ implies
\begin{equation}
\frac{y_i}{\overline x_i^2}>\frac{y_{i+1}}{\overline x_{i+1}^2}
\end{equation}
and thus $\overline x$ cannot be the optimal solution.
:::

@thm-pava can be used to replace problem $\mathbf{P}$ of size $n$ with a problem $\tilde{\mathbf{P}}$of the 
same type, but of size $n-1$. If $y_i\geq y_{i+1}$ we remove these two $y$-values and put the single value 
$$
\tilde y_i:=\frac{w_iy_i+w_{i+1}y_{i+1}}{w_i+w_{i+1}}
$$
in their place. This new value gets the weight $\tilde w_i:=w_i+w_{i+1}$. 

::: {#thm-monreg}
Suppose $\hat x$ solves the monotone regression problem for the weighted least squares norm
$$
\hat x:=\mathop{\text{argmin}}_{x_1\leq\cdots\leq x_n}(x-y)'W(x-y)
$$
then
$$
\overline x_i=\frac{\sqrt{\hat x_i}}{\sum_{k=1}^nw_k\sqrt{\hat x_k}}
$$
solves problem $\mathbf{P}$.
:::
::: {.proof}
We use @thm-pava repeatedly until we have a problem $\mathbf{P}$ of size $r$ for which
$\tilde y_1<\cdots<\tilde y_r$. We can use the same sequence of pooling adjacent
violators as in least squares monotone regression, and we find the same weighted average pooled
values in each step. When we have reduced the problem to a strictly increasing
$\tilde y$ sequence we apply @lem-fine (and if $r=1$ we apply @lem-one). We then
expand each block again to a length equal to the number of averaged elements.
:::

A small example illustrates the PAVA variation. Start with $y$ equal to
$(1,2,3,1,2,3,5,1)$ and $w$ equal to $(1,1,1,1,1,1,1,1)$.
Merge elements 3 and 4 to get $y=(1,2,2,2,3,5,1)$
and $w=(1,1,2,1,1,1,1)$. Now merge elements 2, 3, and 4
to get $y=(1,2,3,5,1)$ and $w=(1,4,1,1,1)$. Merge 4 and 5
to get $y=(1,2,3,3)$ and $w=(1,4,1,2)$ and finally merge
3 and 4 to get $y=(1,2,3)$ and $w=(1,4,3)$. Thus
$$
\overline x=\frac{1}{1+4\sqrt{2}+3\sqrt{3}}(1, \sqrt{2}, \sqrt{2}, \sqrt{2}, \sqrt{2}, \sqrt{3}, \sqrt{3}, \sqrt{3})
$$
Remember that in the context of Sammon Mapping the least squares monotone regression has to be done on the squared distances.

# Examples

## Results

```{r ekman, echo = FALSE}
hen <- smacofSSSammonC(ekmanData, ordinal = FALSE, verbose = FALSE, eps = 1e-6)
heo <- smacofSSSammonC(ekmanData, ordinal = TRUE, verbose = FALSE, eps = 1e-6)
```
Numerical elastic scaling takes `r hen$niter` iterations to arrive at stress `r formatC(hen$stress, digits = 10, format = "f")`. For ordinal
we need `r heo$niter` iterations for stress `r formatC(heo$stress,digits = 10, format = "f")`. Since the ordinal fit is very good the
sqrt-stress of \eqref{eq-sqstress} should be close to Sammon stress. It is `r formatC(4 * sum((sqrt(heo$dhat) - sqrt(heo$confdist))^2), digits = 10, format = "f")`.

```{r morse, echo = FALSE}
hmn <- smacofSSSammonC(morseData, ordinal = FALSE, verbose = FALSE, eps = 1e-6)
hmo <- smacofSSSammonC(morseData, ordinal = TRUE, verbose = FALSE, eps = 1e-6)
```

For the Morse code data numerical elastic scaling takes `r hmn$niter` iterations to arrive at stress `r hmn$stress`, for ordinal we need `r hmo$niter` iterations for stress `r hmo$stress`. Because the fit is much worse than
in the Ekman case we do not expect sqrt-stress to be close to Sammon stress. For the ordinal case final sqrt-stress is `r 4 * sum((sqrt(hmo$dhat) - sqrt(hmo$confdist))^2)`.

## Timing

The results of a comparison of the R and C implementations with microbenchmark (@mersmann_24) yields the times in microseconds in the following table.

```{r  mekman, echo = FALSE, cache = TRUE}
xinit <- smacofTorgerson(ekmanData, ndim = 2)$conf
suppressWarnings(microbenchmark(
  smacofSSSammonR(ekmanData, xinit = xinit, eps = 1e-6, itmax = 1000, ordinal = FALSE, weighted = FALSE, verbose = FALSE),
  smacofSSSammonC(ekmanData, xinit = xinit, eps = 1e-6, itmax = 1000, ordinal = FALSE, weighted = FALSE, verbose = FALSE),
  smacofSSSammonR(ekmanData, xinit = xinit, eps = 1e-6, itmax = 1000, ordinal = TRUE, weighted = FALSE, verbose = FALSE),
  smacofSSSammonC(ekmanData, xinit = xinit, eps = 1e-6, itmax = 1000, ordinal = TRUE, weighted = FALSE, verbose = FALSE)))
```

```{r microekman, echo = FALSE}
a<-matrix(c(40.13, 1.44, 163.52, 4.06, 41.66, 1.53, 168.14, 4.39, 
            44.38, 1.60, 172.10, 4.57, 43.84, 1.60, 169.50, 4.46,
            45.11, 1.64, 172.28, 4.71, 109.41, 1.81, 242.27, 7.37),
          4, 6)
colnames(a) <- c("min", "lq", "mean", "median", "uq", "max")
row.names(a) <- c("R/Numerical", "C/Numerical", "R/Ordinal", "C/Ordinal")
a
```
Using median times we see that that C version is about 40 times as fast as the R version.

```{r mmorse, echo = FALSE, cache = TRUE}
xinit <- smacofTorgerson(morseData, ndim = 2)$conf
suppressWarnings(microbenchmark(
  smacofSSSammonR(morseData, xinit = xinit, eps = 1e-6, itmax = 1000, ordinal = FALSE, weighted = FALSE, verbose = FALSE),
  smacofSSSammonC(morseData, xinit = xinit, eps = 1e-6, itmax = 1000, ordinal = FALSE, weighted = FALSE, verbose = FALSE),
  smacofSSSammonR(morseData, xinit = xinit, eps = 1e-6, itmax = 1000, ordinal = TRUE, weighted = FALSE, verbose = FALSE),
  smacofSSSammonC(morseData, xinit = xinit, eps = 1e-6, itmax = 1000, ordinal = TRUE, weighted = FALSE, verbose = FALSE)))
```

Next, the timings for the Morse data.
```{r micromorse, echo = FALSE}
a<-matrix(c(80.71, 6.38, 711.81, 7.25,
            85.50, 6.62, 744.80, 7.60,
            92.21, 6.77, 767.76, 7.82, 
            87.61, 6.71, 764.32, 7.74,
            91.38, 6.80, 784.45, 7.83,
            162.41, 10.96, 862.41, 11.35),
          4, 6)
colnames(a) <- c("min", "lq", "mean", "median", "uq", "max")
row.names(a) <- c("R/Numerical", "C/Numerical", "R/Ordinal", "C/Ordinal")
a
```
In the numerical case the C version is about 15 times as fast, in the ordinal case
about 100 times. The huge difference in the ordinal case is likely due in large part to the different monotone regression routines used by the R and C programs.

Of course these microbenchmark results depend on the default
parameters of the programs (same in R and C), on the speed of my computer
(Mac Mini with Apple M4 Pro, 64 GB of RAM, Tahoe 26.2, Apple clang 17.0.0), and on my programming habits (same in R and C).

## Plots

```{r compplotE, fig.align = "center", fig.width = 8, fig.height = 8, echo = FALSE}
par(pty="s")
setwd("/Users/deleeuw/Desktop/recents/sammon")
source("smacofSSSammonC.R")
source("../smacofFlat/smacofSSdata/ekmanData.R")
h1 <- smacofSSSammonC(ekmanData, ordinal = TRUE, verbose = FALSE)
smacofShepardPlot(h1, main = "Shepard Plot Ekman, Sammon")
dyn.unload("smacofSSSammon.so")
setwd("/Users/deleeuw/Desktop/recents/smacofFlat")
source("smacofSS.R")
h2 <- smacofSS(ekmanData, ordinal = TRUE, verbose = FALSE)
smacofShepardPlot(h2, main = "Shepard Plot Ekman, Kruskal")
```
```{r compplotM, fig.align = "center", fig.width = 8, fig.height = 8, echo = FALSE}
par(pty="s")
setwd("/Users/deleeuw/Desktop/recents/sammon")
source("smacofSSSammonC.R")
source("../smacofFlat/smacofSSdata/morseData.R")
h1 <- smacofSSSammonC(morseData, ordinal = TRUE, verbose = FALSE)
smacofShepardPlot(h1, main = "Shepard Plot Morse, Sammon")
dyn.unload("smacofSSSammon.so")
setwd("/Users/deleeuw/Desktop/recents/smacofFlat")
source("smacofSS.R")
h2 <- smacofSS(morseData, ordinal = TRUE, verbose = FALSE)
smacofShepardPlot(h2, main = "Shepard Plot Morse, Kruskal")
```
\sectionbreak

# References
